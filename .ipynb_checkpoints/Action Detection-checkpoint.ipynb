{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2fb96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6c0154",
   "metadata": {},
   "source": [
    "# Keypoints using Mediapipe\n",
    "\n",
    "Here we check if we have our web cam access. Then we add an addition layer showing keypoints in our body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52a4e3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistics = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Layer drawing utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fa50a2",
   "metadata": {},
   "source": [
    "Create a function so that our while loop does not get cluttered. We pass a image frame and a model, this model processes the frame and detects the keypoints found. This function then returns the frame and keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23c15162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    result = model.process(image)\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a6ad3e",
   "metadata": {},
   "source": [
    "Function to draw the detected landmarks upon a given frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44360bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistics.FACEMESH_TESSELATION)\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistics.POSE_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistics.HAND_CONNECTIONS)\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistics.HAND_CONNECTIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c708f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_styled_landmarks(image, results):\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistics.FACEMESH_TESSELATION, \n",
    "                              None,\n",
    "                              mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistics.POSE_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                              mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistics.HAND_CONNECTIONS,\n",
    "                              mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=3, circle_radius=3),\n",
    "                              mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1, circle_radius=1)\n",
    "                             )\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistics.HAND_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=3, circle_radius=3),\n",
    "                              mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=1, circle_radius=1)\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "60e6f659",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialise mediapipe model\n",
    "with mp_holistics.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.5) as holistic:\n",
    "    while cap.isOpened():\n",
    "\n",
    "        # Read our feed\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Perform keypoint detection\n",
    "        image, result = mediapipe_detection(frame, holistic)\n",
    "        \n",
    "        # Draw landmarks on the frame\n",
    "        draw_styled_landmarks(image, result)\n",
    "\n",
    "        # Show to screen ('title', frame)\n",
    "        cv2.imshow('OpenCV feed', image)\n",
    "\n",
    "        # Catch event when we want to quit (q)\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7e2532",
   "metadata": {},
   "source": [
    "# Extract Keypoint values\n",
    "\n",
    "Extracting and processing keypoint values into a format that we can use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "992f7988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x: 0.4553624987602234\n",
       "y: 0.5731104016304016\n",
       "z: -1.2322654724121094\n",
       "visibility: 0.9997774362564087\n",
       ", x: 0.48357701301574707\n",
       "y: 0.48920854926109314\n",
       "z: -1.185476541519165\n",
       "visibility: 0.9994886517524719\n",
       ", x: 0.5012804865837097\n",
       "y: 0.48758044838905334\n",
       "z: -1.1852549314498901\n",
       "visibility: 0.9995299577713013\n",
       ", x: 0.5211310982704163\n",
       "y: 0.48568785190582275\n",
       "z: -1.1856610774993896\n",
       "visibility: 0.9994864463806152\n",
       ", x: 0.41371744871139526\n",
       "y: 0.4903739094734192\n",
       "z: -1.1987465620040894\n",
       "visibility: 0.9995215535163879\n",
       ", x: 0.387626975774765\n",
       "y: 0.49075374007225037\n",
       "z: -1.19772207736969\n",
       "visibility: 0.9994756579399109\n",
       ", x: 0.3643929064273834\n",
       "y: 0.492742121219635\n",
       "z: -1.197809100151062\n",
       "visibility: 0.9994405508041382\n",
       ", x: 0.5464252233505249\n",
       "y: 0.5014891028404236\n",
       "z: -0.7554653286933899\n",
       "visibility: 0.9996281266212463\n",
       ", x: 0.3289526104927063\n",
       "y: 0.513339638710022\n",
       "z: -0.7723501920700073\n",
       "visibility: 0.999731183052063\n",
       ", x: 0.4904845654964447\n",
       "y: 0.6480852365493774\n",
       "z: -1.0467863082885742\n",
       "visibility: 0.9998200535774231\n",
       ", x: 0.4116212725639343\n",
       "y: 0.6599267721176147\n",
       "z: -1.0562195777893066\n",
       "visibility: 0.9998594522476196\n",
       ", x: 0.7295318245887756\n",
       "y: 0.8722823858261108\n",
       "z: -0.41332167387008667\n",
       "visibility: 0.9993537068367004\n",
       ", x: 0.14036783576011658\n",
       "y: 0.8766050934791565\n",
       "z: -0.5147982835769653\n",
       "visibility: 0.9993140697479248\n",
       ", x: 0.8356760144233704\n",
       "y: 1.3431724309921265\n",
       "z: -0.30602025985717773\n",
       "visibility: 0.12432791292667389\n",
       ", x: -0.05466969683766365\n",
       "y: 1.3256148099899292\n",
       "z: -0.44106441736221313\n",
       "visibility: 0.2633306682109833\n",
       ", x: 0.8295119404792786\n",
       "y: 1.7214335203170776\n",
       "z: -0.6501107811927795\n",
       "visibility: 0.034946147352457047\n",
       ", x: -0.015439462848007679\n",
       "y: 1.7240417003631592\n",
       "z: -0.8886206746101379\n",
       "visibility: 0.03783862292766571\n",
       ", x: 0.866174578666687\n",
       "y: 1.848519206047058\n",
       "z: -0.7829697728157043\n",
       "visibility: 0.04557840898633003\n",
       ", x: -0.03724135458469391\n",
       "y: 1.860177755355835\n",
       "z: -1.0194919109344482\n",
       "visibility: 0.04287811741232872\n",
       ", x: 0.8154230117797852\n",
       "y: 1.8452142477035522\n",
       "z: -0.8441388010978699\n",
       "visibility: 0.07807096093893051\n",
       ", x: 0.026295650750398636\n",
       "y: 1.8576161861419678\n",
       "z: -1.1210134029388428\n",
       "visibility: 0.07034722715616226\n",
       ", x: 0.7884580492973328\n",
       "y: 1.7965445518493652\n",
       "z: -0.6995183229446411\n",
       "visibility: 0.0824120044708252\n",
       ", x: 0.044706784188747406\n",
       "y: 1.805023193359375\n",
       "z: -0.9496186971664429\n",
       "visibility: 0.07379234582185745\n",
       ", x: 0.6104379892349243\n",
       "y: 1.7668166160583496\n",
       "z: -0.04293954372406006\n",
       "visibility: 0.0003742955159395933\n",
       ", x: 0.24232745170593262\n",
       "y: 1.7840304374694824\n",
       "z: 0.04977831989526749\n",
       "visibility: 0.0003282906545791775\n",
       ", x: 0.6146668791770935\n",
       "y: 2.5109472274780273\n",
       "z: 0.07673240453004837\n",
       "visibility: 0.00027296351618133485\n",
       ", x: 0.2775038182735443\n",
       "y: 2.5297610759735107\n",
       "z: 0.09225548058748245\n",
       "visibility: 9.248676360584795e-05\n",
       ", x: 0.6334560513496399\n",
       "y: 3.189009189605713\n",
       "z: 0.9200512766838074\n",
       "visibility: 2.126493382093031e-05\n",
       ", x: 0.3121359944343567\n",
       "y: 3.191775321960449\n",
       "z: 0.685922384262085\n",
       "visibility: 1.6784092622401658e-06\n",
       ", x: 0.6421446800231934\n",
       "y: 3.308093309402466\n",
       "z: 0.9672104120254517\n",
       "visibility: 2.298024446645286e-05\n",
       ", x: 0.31009480357170105\n",
       "y: 3.3111929893493652\n",
       "z: 0.7312028408050537\n",
       "visibility: 4.9894110816239845e-06\n",
       ", x: 0.5897144675254822\n",
       "y: 3.419884204864502\n",
       "z: 0.2960352301597595\n",
       "visibility: 2.423901059955824e-05\n",
       ", x: 0.37186843156814575\n",
       "y: 3.407871723175049\n",
       "z: -0.05632958933711052\n",
       "visibility: 1.086672637029551e-05\n",
       "]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.pose_landmarks.landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9cf80583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(result):\n",
    "    pose = np.array([[landmark.x, landmark.y, landmark.z, landmark.visibility] for landmark in result.pose_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[landmark.x, landmark.y, landmark.z] for landmark in result.face_landmarks.landmark]).flatten() if result.face_landmarks else np.zeros(468*3)\n",
    "    left_hand = np.array([[landmark.x, landmark.y, landmark.z] for landmark in result.left_hand_landmarks.landmark]).flatten() if result.left_hand_landmarks else np.zeros(21*3)\n",
    "    right_hand = np.array([[landmark.x, landmark.y, landmark.z] for landmark in result.right_hand_landmarks.landmark]).flatten() if result.right_hand_landmarks else np.zeros(21*3)\n",
    "    \n",
    "    return np.concatenate([pose, face, left_hand, right_hand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "02d778e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_test = extract_keypoints(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5a2db05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4553625   0.5731104  -1.23226547 ...  0.          0.\n",
      "  0.        ]\n"
     ]
    }
   ],
   "source": [
    "print(result_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc06519",
   "metadata": {},
   "source": [
    "# Setup folders for data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "651ff3fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'ntpath' from 'C:\\\\Users\\\\LENOVO\\\\anaconda3\\\\envs\\\\tensorflow\\\\lib\\\\ntpath.py'>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6894296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path for exported data (numpy arrays)\n",
    "DATA_PATH = os.path.join('Keypoint Data')\n",
    "\n",
    "# Actions that we try to detect\n",
    "# actions = np.array(['Hello', 'Thanks', 'I Love You'])\n",
    "actions = np.array(['Hello'])\n",
    "no_sequences = 10\n",
    "sequence_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdc053dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in actions:\n",
    "    for seq in range(no_sequences):\n",
    "        try:\n",
    "            os.makedirs(os.path.join(DATA_PATH, action, str(seq)))\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f150aa7",
   "metadata": {},
   "source": [
    "# Collect keypoint data for Training and Testing\n",
    "\n",
    "Collect live feed and store it in the created folders.\n",
    "\n",
    "Collect a sample for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "af1d77c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialise mediapipe model\n",
    "with mp_holistics.Holistic(min_detection_confidence=0.7, min_tracking_confidence=0.5) as holistic:\n",
    "    # Loop each action\n",
    "    for action in actions:\n",
    "        # Collect a defined no of sample videos\n",
    "        for seq in range(no_sequences):\n",
    "            # Collect defined no of frames per video\n",
    "            for frame_no in range(sequence_length):\n",
    "\n",
    "                # Read our feed\n",
    "                ret, frame = cap.read()\n",
    "\n",
    "                # Perform keypoint detection\n",
    "                image, result = mediapipe_detection(frame, holistic)\n",
    "\n",
    "                # Draw landmarks on the frame\n",
    "                draw_styled_landmarks(image, result)\n",
    "                \n",
    "                # Apply wait logic\n",
    "                if frame_no == 0:\n",
    "                    cv2.putText(image, 'STARTING COLLECTION', (120, 200), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 4 ,cv2.LINE_AA)\n",
    "                    cv2.putText(image, 'Collecting Video {} for {}'.format(seq, action), (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1 ,cv2.LINE_AA)\n",
    "                    # Show to screen ('title', frame)\n",
    "                    cv2.imshow('OpenCV feed', image)\n",
    "                    cv2.waitKey(2000)\n",
    "                else:\n",
    "                    cv2.putText(image, 'Collecting Video {} for {}'.format(seq, action), (15, 12), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1 ,cv2.LINE_AA)\n",
    "                    # Show to screen ('title', frame)\n",
    "                    cv2.imshow('OpenCV feed', image)\n",
    "                    \n",
    "                # Export keypoints to folders\n",
    "                keypoints = extract_keypoints(result)\n",
    "                npy_path = os.path.join(DATA_PATH, action, str(seq), str(frame_no))\n",
    "                np.save(npy_path, keypoints)\n",
    "\n",
    "                # Catch event when we want to quit (q)\n",
    "                if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "                    break\n",
    "\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d26f49a",
   "metadata": {},
   "source": [
    "# Preprocess data and Create labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1235e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "868a77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {label:num for num, label in  enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "586f9628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Hello': 0, 'Thanks': 1, 'I Love You': 2}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "47d1e2a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [78]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m window \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m frame_no \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(sequence_length):\n\u001b[1;32m----> 6\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_no\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     window\u001b[38;5;241m.\u001b[39mappend(res)\n\u001b[0;32m      8\u001b[0m sequences\u001b[38;5;241m.\u001b[39mappend(window)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\lib\\npyio.py:413\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode)\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 413\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    414\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    416\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\lib\\format.py:741\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[0;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[0;32m    739\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[1;32m--> 741\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject arrays cannot be loaded when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    742\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    744\u001b[0m         pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for seq in range(no_sequences):\n",
    "        window = []\n",
    "        for frame_no in range(sequence_length):\n",
    "            res = np.load(os.path.join(DATA_PATH, action, str(seq), \"{}.npy\".format(frame_no)))\n",
    "            window.append(res)\n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d066fd37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
